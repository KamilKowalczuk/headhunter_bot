import os
import re
import requests
import json
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from sqlalchemy.orm import Session
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# Importy z aplikacji
from app.database import Lead, GlobalCompany
from app.tools import verify_email_domain, get_main_domain_url
from app.schemas import CompanyResearch

# Konfiguracja loggera
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("researcher")

load_dotenv()

# Konfiguracja API
gemini_key = os.getenv("GEMINI_API_KEY")
firecrawl_key = os.getenv("FIRECRAWL_API_KEY")

if not firecrawl_key:
    raise ValueError("‚ùå CRITICAL: Brak FIRECRAWL_API_KEY w .env.")

# Model AI - Zwiƒôkszamy temperaturƒô minimalnie dla kreatywno≈õci w 'icebreaker'
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.1, google_api_key=gemini_key)
structured_llm = llm.with_structured_output(CompanyResearch)

# --- NARZƒòDZIA POMOCNICZE (SNIPER TOOLS) ---

def extract_emails_via_regex(text: str) -> list:
    """Szybki regex do wy≈Çapywania maili przed AI."""
    if not text: return []
    # Ulepszony regex (odrzuca pliki graficzne w ≈õrodku maila)
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    found = re.findall(email_pattern, text)
    unique = list(set(email.lower() for email in found))
    
    clean = []
    for email in unique:
        # Filtry antyspamowe/antyassetowe
        if email.endswith(('.png', '.jpg', '.jpeg', '.gif', '.css', '.js', '.svg', '.woff')): continue
        if any(x in email for x in ['sentry', 'noreply', 'no-reply', 'example', 'domain', 'email']): continue
        if len(email) < 5 or len(email) > 60: continue
        clean.append(email)
    return clean

class TitanScraper:
    """Klient Firecrawl z obs≈ÇugƒÖ b≈Çƒôd√≥w i timeout√≥w."""
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.firecrawl.dev/v1"
        self.headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    def scrape(self, url, check_hiring=False):
        """Pobiera tre≈õƒá. Opcjonalnie szuka s≈Ç√≥w kluczowych rekrutacji."""
        endpoint = f"{self.base_url}/scrape"
        payload = {
            "url": url, 
            "formats": ["markdown"],
            "onlyMainContent": True, # Oszczƒôdno≈õƒá token√≥w - tylko miƒôso
            "timeout": 15000
        }
        try:
            response = requests.post(endpoint, headers=self.headers, json=payload, timeout=20)
            if response.status_code == 200:
                data = response.json()
                content = data.get('data', {}).get('markdown', "")
                return content
            elif response.status_code == 429:
                logger.warning(f"Rate limit Firecrawl na {url}")
                return ""
            return ""
        except Exception as e:
            logger.error(f"B≈ÇƒÖd scrapowania {url}: {e}")
            return ""

    def map_site(self, url):
        """Mapuje stronƒô w poszukiwaniu podstron."""
        endpoint = f"{self.base_url}/map"
        payload = {"url": url, "search": "contact about team career kontakt o-nas zespol kariera"}
        try:
            response = requests.post(endpoint, headers=self.headers, json=payload, timeout=15)
            if response.status_code == 200:
                data = response.json()
                return data.get('links', []) if 'links' in data else data.get('data', {}).get('links', [])
            return []
        except:
            return []

scraper = TitanScraper(firecrawl_key)

def _parallel_scrape(urls: list) -> str:
    """
    R√≥wnoleg≈Çe pobieranie tre≈õci z wielu URLi.
    To jest GAME CHANGER dla wydajno≈õci.
    """
    full_content = ""
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_url = {executor.submit(scraper.scrape, url): url for url in urls}
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                data = future.result()
                if data and len(data) > 50:
                    # Dodajemy nag≈Ç√≥wek, ≈ºeby AI wiedzia≈Ço skƒÖd jest tekst
                    section_name = "STRONA G≈Å√ìWNA"
                    if "contact" in url or "kontakt" in url: section_name = "KONTAKT"
                    elif "about" in url or "o-nas" in url: section_name = "O NAS"
                    elif "career" in url or "kariera" in url: section_name = "KARIERA/PRACA"
                    
                    full_content += f"\n\n=== {section_name} ({url}) ===\n{data[:15000]}" # Limit na podstronƒô
            except Exception as e:
                logger.error(f"B≈ÇƒÖd w wƒÖtku dla {url}: {e}")
    return full_content

def _get_content_titan_strategy(url: str) -> str:
    """Strategia Zwiadu: Mapowanie -> Wyb√≥r Cel√≥w -> R√≥wnoleg≈Çy Atak."""
    print(f"      üî• [TITAN] Rozpoczynam skanowanie domeny: {url}")
    
    # 1. Mapowanie (szybkie)
    links = scraper.map_site(url)
    pages_to_scrape = [url] # Zawsze strona g≈Ç√≥wna
    
    # 2. Inteligentny wyb√≥r cel√≥w
    if links:
        # Priorytety: Kontakt > O nas > Kariera (szukanie sygna≈Ç√≥w zakupowych)
        keywords_priority = {
            "kontakt": 1, "contact": 1,
            "o-nas": 2, "about": 2, "team": 2, "zespol": 2,
            "kariera": 3, "career": 3, "jobs": 3, "praca": 3
        }
        
        # Unikalne linki, sortowanie po priorytecie
        scored_links = []
        seen = set([url])
        
        for link in links:
            if link in seen: continue
            if any(ext in link.lower() for ext in ['.jpg', '.png', '.pdf', '.css', 'wp-content']): continue
            
            score = 10 # Domy≈õlnie niski priorytet
            for key, val in keywords_priority.items():
                if key in link.lower():
                    score = val
                    break
            
            if score < 10: # Tylko je≈õli znale≈∫li≈õmy s≈Çowo kluczowe
                scored_links.append((score, link))
                seen.add(link)

        # Sortujemy (1 najni≈ºsze = najwa≈ºniejsze) i bierzemy max 3 dodatkowe podstrony
        scored_links.sort(key=lambda x: x[0])
        top_links = [x[1] for x in scored_links[:3]]
        pages_to_scrape.extend(top_links)
        
        print(f"         üéØ Cele taktyczne: {[u.split('/')[-1] for u in pages_to_scrape[1:]]}")
    else:
        # Fallback manualny
        base = url.rstrip('/')
        pages_to_scrape.extend([f"{base}/kontakt", f"{base}/o-nas"])

    # 3. R√≥wnoleg≈Çe pobieranie (B≈ÅYSKAWICZNE)
    return _parallel_scrape(pages_to_scrape)

def analyze_lead(session: Session, lead_id: int):
    """
    G≈Ç√≥wna funkcja analityczna.
    """
    lead = session.query(Lead).filter(Lead.id == lead_id).first()
    if not lead: return

    company = lead.company
    print(f"\n   üîé [RESEARCHER] Analizujƒô firmƒô: {company.name}")
    
    # Normalizacja URL
    target_url = get_main_domain_url(company.domain)
    if not target_url.startswith("http"): target_url = "https://" + target_url

    # 1. POBIERANIE DANYCH (ASYNC LOGIC WRAPPED)
    content = _get_content_titan_strategy(target_url)
    
    if not content:
        print(f"      ‚ùå PUSTY ZWIAD. Oznaczam do rƒôcznego sprawdzenia.")
        lead.status = "MANUAL_CHECK"
        session.commit()
        return

    # 2. EKSTRAKCJA REGEX (SAFEGUARD)
    regex_emails = extract_emails_via_regex(content)
    if regex_emails:
        print(f"      üëÄ Regex znalaz≈Ç: {regex_emails}")

    # 3. ANALIZA SEMANTYCZNA AI (M√ìZG)
    print(f"      üß† Uruchamiam Gemini 2.0 (Business Intelligence)...")
    
    system_prompt = f"""
    Jeste≈õ elitarnym analitykiem sprzeda≈ºy B2B (Agency OS).
    Twoim celem jest przygotowanie "amunicji" dla copywritera, aby sprzedaƒá us≈Çugi tej firmie.
    
    DANE WEJ≈öCIOWE:
    Strona WWW klienta (sekcje Home, Kontakt, O nas, Kariera).
    
    ZADANIE:
    1. Zidentyfikuj **Stack Technologiczny** (jakich narzƒôdzi u≈ºywajƒÖ? Wordpress? React? HubSpot?).
    2. Znajd≈∫ **Sygna≈Çy Zakupowe (Hiring Signals)**. Czy rekrutujƒÖ handlowc√≥w? Programist√≥w? To oznacza, ≈ºe majƒÖ bud≈ºet i potrzeby.
    3. Znajd≈∫ **Decydent√≥w**. Imiona, nazwiska, stanowiska.
    4. Napisz **ICEBREAKER**. Jedno, genialne zdanie, kt√≥re udowadnia, ≈ºe zrobili≈õmy research. Np. "Gratulujƒô nagrody X", "Widzia≈Çem, ≈ºe szukacie Head of Sales".
    5. Wybierz najlepszy **E-MAIL**.
    
    Wskaz√≥wka od systemu (Regex): {', '.join(regex_emails) if regex_emails else 'Brak'}
    Je≈õli Regex znalaz≈Ç maila, zweryfikuj go kontekstowo i u≈ºyj.
    """
    
    chain = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{text}")]).pipe(structured_llm)
    
    try:
        # Przekazujemy tekst, ale ucinamy go bezpiecznie do okna kontekstu (ok 60k znak√≥w dla pewno≈õci)
        research = chain.invoke({"text": content[:60000]})
    except Exception as e:
        print(f"      ‚ùå B≈ÇƒÖd LLM: {e}")
        lead.status = "MANUAL_CHECK"
        session.commit()
        return

    # 4. LOGIKA WYBORU MAILA (SCORING)
    valid_email = None
    all_candidates = list(set((research.contact_emails or []) + regex_emails))
    
    def score_email(email):
        s = 0
        e = email.lower()
        # Bonusy
        if any(x in e for x in ['prezes', 'ceo', 'owner', 'dyrektor', 'head']): s += 10
        if '.' in e.split('@')[0]: s += 5 # Format imie.nazwisko
        if any(x in e for x in ['hello', 'contact', 'biuro', 'info']): s += 2
        # Kary
        if any(x in e for x in ['kariera', 'jobs', 'rekrutacja', 'no-reply', 'abuse']): s -= 100
        if not verify_email_domain(e): s -= 50 # Sprawdzenie DNS
        return s

    if all_candidates:
        # Sortuj malejƒÖco po wyniku
        scored_emails = sorted([(e, score_email(e)) for e in all_candidates], key=lambda x: x[1], reverse=True)
        print(f"      üìß Scoring maili: {scored_emails}")
        
        best_email, score = scored_emails[0]
        if score > -20: # Pr√≥g akceptacji
            valid_email = best_email
        else:
            print("      ‚ö†Ô∏è Wszystkie maile odrzucone (spam/kariera/dns).")

    # 5. AKTUALIZACJA BAZY (COMMIT)
    company.tech_stack = research.tech_stack
    company.decision_makers = research.decision_makers
    company.industry = research.target_audience # Czƒôsto ICP klienta m√≥wi o jego bran≈ºy
    company.last_scraped_at = datetime.utcnow()
    
    # Budujemy potƒô≈ºne podsumowanie dla Writera
    hiring_info = f"REKRUTUJƒÑ: {', '.join(research.hiring_signals)}" if research.hiring_signals else "Brak rekrutacji."
    lead.ai_analysis_summary = (
        f"ICEBREAKER: {research.icebreaker}\n"
        f"SUMMARY: {research.summary}\n"
        f"ICP: {research.target_audience}\n"
        f"{hiring_info}\n"
        f"PAIN POINTS: {research.pain_points_or_opportunities}"
    )
    
    if valid_email:
        lead.target_email = valid_email
        lead.status = "ANALYZED" # Gotowy dla Writera
        lead.ai_confidence_score = 95 # Wysokie zaufanie po g≈Çƒôbokim researchu
        print(f"      ‚úÖ SUKCES: Lead gotowy. Target: {valid_email}")
    else:
        lead.status = "MANUAL_CHECK" # Cz≈Çowiek musi poszukaƒá na LinkedIn
        lead.ai_confidence_score = 20
        print(f"      ‚ö†Ô∏è PARTIAL: Mamy dane, ale brak maila. Do rƒôcznej weryfikacji.")

    session.commit()