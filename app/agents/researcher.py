import os
import re
import httpx  # <--- ZMIANA: httpx zamiast requests
import json
import logging
import html
import asyncio
# from concurrent.futures import ThreadPoolExecutor, as_completed # <--- USUNIƒòTE (ZastƒÖpione przez asyncio.gather)
from datetime import datetime
from sqlalchemy.orm import Session
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# Importy z aplikacji
from app.database import Lead, GlobalCompany
from app.tools import verify_email_domain, get_main_domain_url
from app.schemas import CompanyResearch

# Konfiguracja loggera
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("researcher")

load_dotenv()

# Konfiguracja API
gemini_key = os.getenv("GEMINI_API_KEY")
firecrawl_key = os.getenv("FIRECRAWL_API_KEY")

if not firecrawl_key:
    # Nie rzucamy b≈Çƒôdu krytycznego przy imporcie, tylko logujemy, ≈ºeby apka nie pad≈Ça
    logger.error("‚ùå CRITICAL: Brak FIRECRAWL_API_KEY w .env. Researcher nie zadzia≈Ça.")

# Model AI
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.1, google_api_key=gemini_key)
structured_llm = llm.with_structured_output(CompanyResearch)

# --- NARZƒòDZIA POMOCNICZE (SNIPER TOOLS) ---

def extract_emails_from_html(raw_html: str) -> list:
    """Ekstrakcja z BRUDNEGO HTMLa (X-RAY)."""
    if not raw_html: return []
    
    text = html.unescape(raw_html)
    emails = []
    
    # 1. Linki mailto (Priorytet)
    mailto_pattern = r'mailto:([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
    emails.extend(re.findall(mailto_pattern, text))
    
    # 2. Tekst
    text_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails.extend(re.findall(text_pattern, text))
    
    unique = list(set(e.lower() for e in emails))
    clean = []
    for email in unique:
        if email.endswith(('.png', '.jpg', '.jpeg', '.gif', '.css', '.js', '.svg', '.woff', '.webp', '.mp4')): continue
        if any(x in email for x in ['sentry', 'noreply', 'no-reply', 'example', 'domain', 'email.com', 'bootstrap', 'react']): continue
        if len(email) < 5 or len(email) > 60: continue
        clean.append(email)
        
    return clean

class TitanScraper:
    """Klient Firecrawl - Tryb Async (HTTPX)."""
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.firecrawl.dev/v1"
        self.headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    async def scrape(self, url):  # <--- ZMIANA: async def
        """Pobiera HTML (dla Regexa) i Markdown (dla AI)."""
        if not self.api_key: return None
        
        endpoint = f"{self.base_url}/scrape"
        payload = {
            "url": url, 
            "formats": ["markdown", "html"], 
            "onlyMainContent": False, # WA≈ªNE: Pobieramy stopki!
            "timeout": 20000,
            "excludeTags": ["script", "style", "video", "canvas"] 
        }
        
        # U≈ºywamy httpx.AsyncClient dla nieblokujƒÖcych zapyta≈Ñ
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(endpoint, headers=self.headers, json=payload)
                if response.status_code == 200:
                    data = response.json().get('data', {})
                    if not data.get('markdown') and not data.get('html'):
                        return None
                    return {
                        "markdown": data.get('markdown', ""),
                        "html": data.get('html', "")
                    }
                return None
            except Exception as e:
                logger.error(f"B≈ÇƒÖd scrapowania {url}: {e}")
                return None

    async def map_site(self, url): # <--- ZMIANA: async def
        """Mapuje stronƒô."""
        if not self.api_key: return []
        
        endpoint = f"{self.base_url}/map"
        payload = {"url": url, "search": "contact about team career kontakt o-nas zespol kariera"}
        
        async with httpx.AsyncClient(timeout=15.0) as client:
            try:
                response = await client.post(endpoint, headers=self.headers, json=payload)
                if response.status_code == 200:
                    data = response.json()
                    return data.get('links', []) or data.get('data', {}).get('links', [])
                return []
            except:
                return []

scraper = TitanScraper(firecrawl_key)

async def _parallel_scrape(urls: list) -> dict: # <--- ZMIANA: async def
    """WielowƒÖtkowe pobieranie (Async Gather)."""
    combined_markdown = ""
    all_html_emails = []
    
    urls = list(set(urls))
    
    print(f"         üöÄ Uruchamiam {len(urls)} zada≈Ñ async scrapingowych...")
    
    # Zastƒôpujemy ThreadPoolExecutor przez asyncio.gather (prawdziwa r√≥wnoleg≈Ço≈õƒá IO)
    tasks = [scraper.scrape(url) for url in urls]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    for i, result in enumerate(results):
        url = urls[i]
        
        if isinstance(result, Exception):
            logger.error(f"B≈ÇƒÖd zadania {url}: {result}")
            continue
            
        if result:
            # 1. Regex z HTML
            if result.get("html"):
                found = extract_emails_from_html(result["html"])
                if found:
                    print(f"            üëÄ Znaleziono w HTML ({url}): {found}")
                    all_html_emails.extend(found)
            
            # 2. Markdown dla AI
            md = result.get("markdown", "")
            if len(md) > 50:
                section_name = "STRONA"
                if "contact" in url or "kontakt" in url: section_name = "KONTAKT"
                elif "about" in url or "o-nas" in url: section_name = "O NAS"
                
                combined_markdown += f"\n\n=== {section_name} ({url}) ===\n{md[:15000]}"
                
    return {
        "markdown": combined_markdown,
        "regex_emails": list(set(all_html_emails))
    }

async def _get_content_titan_strategy(url: str) -> dict: # <--- ZMIANA: async def
    """Strategia BULLDOZER: Mapowanie + Wymuszone ≈öcie≈ºki (Async)."""
    print(f"      üî• [TITAN] Cel: {url}")
    
    base_url = url.rstrip('/')
    forced_pages = [
        base_url,
        f"{base_url}/kontakt",
        f"{base_url}/contact",
        f"{base_url}/o-nas",
        f"{base_url}/about"
    ]
    
    # Async Mapowanie
    mapped_links = await scraper.map_site(url)
    final_list = forced_pages.copy()
    
    if mapped_links:
        keywords = ["team", "zespol", "kariera", "career", "praca"]
        interesting = [l for l in mapped_links if any(k in l.lower() for k in keywords)]
        final_list.extend(interesting[:2])

    clean_urls = []
    seen = set()
    for u in final_list:
        if u in seen: continue
        if any(ext in u.lower() for ext in ['.pdf', '.jpg', '.png', '#']): continue
        clean_urls.append(u)
        seen.add(u)

    clean_urls.sort(key=lambda x: 0 if 'kontakt' in x or 'contact' in x else 1)
    target_urls = clean_urls[:5]

    print(f"         üéØ Lista cel√≥w: {[u.split('/')[-1] for u in target_urls]}")
    # Async Scraping
    return await _parallel_scrape(target_urls)

def analyze_lead(session: Session, lead_id: int):
    """
    RESEARCHER V4: BULLDOZER EDITION (Wrapper Synchroniczny).
    Uruchamia asynchroniczny scraping wewnƒÖtrz synchronicznej funkcji.
    """
    lead = session.query(Lead).filter(Lead.id == lead_id).first()
    if not lead: return

    company = lead.company
    client = lead.campaign.client # Pobieramy klienta ≈ºeby sprawdziƒá tryb
    mode = getattr(client, "mode", "SALES") # SALES lub JOB_HUNT

    print(f"\n   üîé [RESEARCHER {mode}] Analiza: {company.name}")
    
    target_url = get_main_domain_url(company.domain)
    if not target_url.startswith("http"): target_url = "https://" + target_url

    # 1. POBIERANIE (Bulldozer Strategy - RUN ASYNC IN SYNC CONTEXT)
    # U≈ºywamy asyncio.run, aby odpaliƒá szybki event loop dla httpx wewnƒÖtrz wƒÖtku roboczego
    try:
        scan_result = asyncio.run(_get_content_titan_strategy(target_url))
    except Exception as e:
        logger.error(f"      ‚ùå B≈ÇƒÖd Async Loop w Research: {e}")
        scan_result = {"markdown": "", "regex_emails": []}
    
    content_md = scan_result["markdown"]
    regex_emails = scan_result["regex_emails"]

    if not content_md and not regex_emails:
        print(f"      ‚ùå PUSTY ZWIAD. Pr√≥ba 404 na wszystkich podstronach.")
        lead.status = "MANUAL_CHECK"
        session.commit()
        return

    # 2. ANALIZA AI (Zale≈ºna od TRYBU)
    print(f"      üß† Gemini analizuje dane...")
    
    regex_hint = ""
    if regex_emails:
        regex_hint = (
            f"ZNALAZ≈ÅEM NASTƒòPUJƒÑCE MAILE W KODZIE HTML (TO SƒÑ FAKTY): {', '.join(regex_emails)}. "
            f"DODAJ JE DO LISTY contact_emails."
        )

    if mode == "JOB_HUNT":
        # --- PROMPT REKRUTACYJNY ---
        system_prompt = f"""
        Jeste≈õ Analitykiem Rynku Pracy IT. Twoim zadaniem jest oceniƒá firmƒô jako potencjalnego PRACODAWCƒò.
        Analizujesz surowƒÖ tre≈õƒá ze strony WWW.
        
        ZADANIA PRIORYTETOWE:
        1. **E-MAIL:** {regex_hint} Szukaj maili do HR, Rekrutacji (kariera@, jobs@, rekrutacja@) LUB do CTO/Team Leader√≥w.
        2. **TECH STACK:** Jakie technologie widaƒá w og≈Çoszeniach o pracƒô lub opisach projekt√≥w? (np. Python, AWS, React).
        3. **HIRING:** Czy majƒÖ zak≈Çadkƒô "Kariera"? Czy szukajƒÖ ludzi? (Nawet je≈õli nie ma Twojego stanowiska).
        4. **DECYDENT:** Szukaj imion: CTO, Head of Engineering, HR Manager, Founder.
        
        CELE:
        - Znajd≈∫ punkty zaczepienia do listu motywacyjnego ("Widzƒô, ≈ºe u≈ºywacie X").
        - Wy≈Çap kulturƒô firmy (Remote/Hybrid?).
        """
    else:
        # --- PROMPT SPRZEDA≈ªOWY (STANDARD) ---
        system_prompt = f"""
        Jeste≈õ analitykiem B2B. Analizujesz surowƒÖ tre≈õƒá HTML/Markdown z kilku podstron firmy.
        
        ZADANIE:
        1. **E-MAIL:** {regex_hint} Szukaj w sekcjach "Kontakt", "Stopka".
        2. Stack Tech & Hiring (Jako sygna≈Ç rozwoju).
        3. Icebreaker (Punkt zaczepienia do sprzeda≈ºy).
        
        Priorytety maili: Imienne > Biuro/Kontakt/Hello > Sprzeda≈º.
        Ignoruj: przyk≈Çadowe domeny, webmaster√≥w, grafikƒô.
        """
    
    try:
        chain = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{text}")]).pipe(structured_llm)
        research = chain.invoke({"text": content_md[:70000]})
    except Exception as e:
        print(f"      ‚ùå B≈ÇƒÖd LLM: {e}")
        if regex_emails:
            print("      ‚ö†Ô∏è LLM Error. Ratujƒô lead mailami z HTML.")
            lead.target_email = regex_emails[0]
            lead.status = "ANALYZED"
            lead.ai_confidence_score = 50
            lead.ai_analysis_summary = "HTML RESCUE MODE. LLM FAILED."
            session.commit()
            return
        lead.status = "MANUAL_CHECK"
        session.commit()
        return

    # 3. MERGE & SCORE (Zale≈ºne od trybu)
    combined_emails = list(set((research.contact_emails or []) + regex_emails))
    
    def score_email(email):
        s = 0
        e = email.lower()
        if mode == "JOB_HUNT":
            # W trybie pracy: HR i Kariera sƒÖ OK, ale CTO/Founder lepsi
            if any(x in e for x in ['kariera', 'jobs', 'rekrutacja', 'hr', 'people']): s += 20
            if any(x in e for x in ['cto', 'tech', 'engineering']): s += 25
            if any(x in e for x in ['ceo', 'founder']): s += 15
        else:
            # W trybie sprzeda≈ºy: Kariera to ≈õmietnik
            if any(x in e for x in ['ceo', 'owner', 'founder', 'prezes']): s += 20
            if any(x in e for x in ['kariera', 'jobs', 'rekrutacja']): s -= 20 # Kara za HR w sprzeda≈ºy
            
        if any(x in e for x in ['biuro', 'info', 'hello', 'kontakt', 'office']): s += 15
        if '.' in e.split('@')[0]: s += 5
        if not verify_email_domain(e): s -= 100
        return s

    valid_email = None
    if combined_emails:
        scored = sorted([(e, score_email(e)) for e in combined_emails], key=lambda x: x[1], reverse=True)
        print(f"      üìß Scoring [{mode}]: {scored}")
        
        best_email, score = scored[0]
        if score > -20:
            valid_email = best_email

    # 4. ZAPIS
    company.tech_stack = research.tech_stack
    company.decision_makers = research.decision_makers
    company.industry = research.target_audience
    company.last_scraped_at = datetime.now()
    
    lead.ai_analysis_summary = (
        f"MODE: {mode}\n"
        f"ICEBREAKER: {research.icebreaker}\n"
        f"SUMMARY: {research.summary}\n"
        f"MAILS: {combined_emails}\n"
        f"HIRING: {research.hiring_signals}\n"
        f"PAIN: {research.pain_points_or_opportunities}"
    )
    
    if valid_email:
        lead.target_email = valid_email
        lead.status = "ANALYZED"
        lead.ai_confidence_score = 95
        print(f"      ‚úÖ SUKCES: {valid_email}")
    else:
        lead.status = "MANUAL_CHECK"
        lead.ai_confidence_score = 15
        print(f"      ‚ö†Ô∏è MANUAL CHECK")

    session.commit()

# --- ASYNC WRAPPER DLA PƒòTLI G≈Å√ìWNEJ ---
async def analyze_lead_async(session: Session, lead_id: int):
    """
    Asynchroniczny wrapper dla researchera.
    Uruchamia ciƒô≈ºki proces scrapowania w osobnym wƒÖtku (przez analyze_lead),
    a wewnƒÖtrz wƒÖtku odpala siƒô mini-loop AsyncIO dla httpx.
    """
    loop = asyncio.get_running_loop()
    await loop.run_in_executor(None, analyze_lead, session, lead_id)